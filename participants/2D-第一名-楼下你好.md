# 完成分割任务的整体流程
## 初赛阶段
- **选择最佳的数据增强方案**
  - 采用滑动窗口
    - 考虑到半监督所提供的数据较少，采用滑动窗口的方式，可以创造出许多“相似且不完全相同”的图片作为训练集，扩充数据量；
    - 每张图片尺寸大小为：320×640，可以设置滑动窗口为320×320，按照步长为32个像素点进行滑动，此处我们就没有必要再去加上resize操作，可以最大程度的保留数据原来的信息；
    - valid和test时候图片的拼接技巧，这里我们采用了一个重叠部分累加1的做法，最后生成的图片小块叠加后，叠加部分取平均值，就可以让图片呈现的效果显得流畅，如下图所示：![Alt text](%E5%9B%BE%E7%89%875.png)
    - 其他数据增强方式,主要采用albumentations库内的数据增强：
      - 随机翻转90度
      - 随机左右翻转
      - 随机上下翻转
      - 随机选择图片的对比度和亮度
      - 随机平移，缩放和旋转
      - 高斯噪声、高斯模糊、运动模糊、随机伽马分布
      - 随机放射变换
      - 正则化
  

- **设计出比较符合赛题的计分函数**
  - 按照比赛计分公式：score = 0.4 × Dice + 0.3 × IOU + 0.3 × (1 - H(d))
  - 该比赛的计分代码采用了monai库中的代码
- **对比各模型，找出得分较高的模型**
  - 用了以下模型做了对比：
    - Unet
    - Unet++
    - SegResNet_8
    - SegResNet_32
    - SegResNet_64
    - SwinUNETR
    - AttentionUnet
    - SegResNetDS
    - DynUNet
  - 通过对比分数以及wandb上正确率曲线，最终确定两个模型进入复赛做融合，分别是
    - SegResNet_64
    - DynUNet

## 复赛阶段
-分析：
复赛阶段给到的有标签数据只有900张，而未标签的数据有2100张，因此可以考虑用900对已经标签的数据对两个不同网络架构的模型训练成两个“老师模型”，再将这两个“老师模型”进行融合，混合900张标签图片和2100张未标签的图片，在融合后的老师模型上得出3000张软标签。其中，2100张未标签的图片，可以充分全面地挖掘两个“老师”潜在的“能力”和“技能”，然后以新的3000张图片作为“参考资料”，分别去训练两个资质与老师相似的“学生”，最后将所有的模型融合成一个模型，达到SOTA的效果。具体操作如下所示：
- 第一步：用900张已经标签的数据集训练两个老师模型，此处通过缩小滑动步长，来增加数据量，滑步由原来的32调整为16![Alt text](%E5%9B%BE%E7%89%871.png)
- 第二步：混合900张标签过的图片和2100张未标签的图片，给两个融合的老师模型制作3000张软标签![Alt text](%E5%9B%BE%E7%89%872.png)
- 第三步：用3000对图片和软标签，重新训练两个学生模型![Alt text](%E5%9B%BE%E7%89%873.png)
- 第四步：将所有的老师模型和学生模型作融合，预测结果并提交![Alt text](%E5%9B%BE%E7%89%874.png)
  
# 部署实验的细节
## 数据部分
  - 数据后缀作成可选，由于在生成软标签的时候，是以".npy"作为后缀保存，为了方便操作，将其直接在config中设为可以调整，另外代码复用的时候，假如images是以jpg或者tiff格式的，也可以随时切换
  - tran_batchsize大小设置，建议按照具体显卡设备容量来设置，这里没有固定的值，每次实验，根据显存容量情况动态调整，尽可能填满显存。
  - valid_batchsize和test_batchsize可以与train_batchsize相同，这个不影响模型效果。
  - 读取图片的时候采用的是opencv-python读取图片的灰度值，代码所示如图：
  ```python
      image = cv2.imread(img_path, 0) # 读取图片灰度值，因此通道数是1
  ```
  - 其他细节，参考“完成分割任务的整体流程”中的介绍
## 模型部分
  - SegResNet_64参数部分设计如下：
  ```python
  from monai.networks.nets.segresnet import SegResNet 
  self.model = SegResNet(
            spatial_dims=2,             # 此处设置是在二维空间
            init_filters=64,            # 此处设为64
            in_channels=CFG.in_chans,   # CFG.in_chans=1 输入通道数是1
            out_channels=CFG.target,    # CFG.target = 1 输出通道数是1
        )
  ```
  - DynUNet参数部分设计如下：
  ```python
  from monai.networks.nets.dynunet import DynUNet
  self.model = DynUNet(
                spatial_dims=2,           # 此处取值为2，说明是在处理二维空间
                in_channels=CFG.in_chans, # 此处实际取值为1
                out_channels=CFG.target,  # 此处实际取值为1
                kernel_size=[[3, 3]] * 5, # 设置五层3×3的卷积
                strides=[[1, 1]] +  [[2, 2]] * 4, # 第一层滑动为1后面四层为2
                upsample_kernel_size=[[2, 2]] * 4,  # 为后四层设置上采样，卷积为2×2
                norm_name=("INSTANCE", {"affine": True}), # 设置正则化
                act_name=("leakyrelu", {"inplace": True, "negative_slope": 0.01}) # 设置激活函数
            )
  
  ```
# 其他部分
  - loss函数部分
    - BCEWithLogitsLoss：训练老师模型的时候采用nn.BCEWithLogitsLoss(),该函数在图像分割类中运用经常会被用到，原理是在nn.BCELoss预测结果的基础上先做了sigmoid后，因此在选择网络的时候，要留意网络内部结果最后一层是否已经做了sigmoid，假如所选的网络已经有了sigmoid操作，后面可以直接接上nn.BCELoss，在具体操作中，有时候就算做了两次sigmoid并不会有影响
    - MSELoss: 在采用软标签训练学生模型的时候，我采用的是nn.MSELoss(),该函数的全拼是均方误差，之所以选择这个函数是想要将学生模型预测的每个像素点尽可能接近于老师模型的软标签，从而达到学习到老师模型掌握到的“能力”。
  - Scheduler部分
    - 学习率优化此处采用了某个算法平台上的工程师提供的代码，链接：https://www.kaggle.com/code/underwearfitting/single-fold-training-of-resnet200d-lb0-965 该学习率策略在原有的GradualWarmupScheduler上做了些调整和优化
    - GradualWarmupScheduler原理是当multiplier=1.0时，学习率lr从0开始增到base_lr为止，当multiplier大于1.0时，学习率lr从base_lr开始增到base_lr*multiplier为止。multiplier不能小于1.0
  - optimizer部分
    - 本实验采用的是AdamW，其中步长设置为3e-3也即是(0.003),weight_decay(权重衰减)设置为1e-5(即0.00001)，weight_decay设置的目的是解决训练过拟合问题
  - 对预测结果采用数据增强TTA
    - 在以上所有的模型中，对已经训练出来的模型，预测上都加了数据增强，也就是一张图会先分别顺时针旋转后，进行预测，预测的结果会按照原来转动的顺序逆向转回去，并叠加后在通道、高、宽的三个维度中的通道维度上取均值，若标记为C, H, W，则经过TTA增强后由(C=1, H=320, W=320)变为(C=4, H=320, W=320),在通道数上取均值后又再次转变成(C=1, H=320, W=320)。
# 结果展示
![Alt text](%E5%9B%BE%E7%89%876.png)
# 参考文献
- U-Net: Convolutional Networks for Biomedical Image Segmentation
- UNet++: A Nested U-Net Architecture for Medical Image Segmentation
- SegResNet based on "3D MRI brain tumor segmentation using autoencoder regularization  <https://arxiv.org/pdf/1810.11654.pdf>"
- Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images <https://arxiv.org/abs/2201.01266>"
- Attention Unet based on Otkay et al. "Attention U-Net: Learning Where to Look for the Pancreas" https://arxiv.org/abs/1804.03999
- SegResNetDS based on "3D MRI brain tumor segmentation using autoencoder regularization <https://arxiv.org/pdf/1810.11654.pdf>"
- UNet (DynUNet) is based on:
    "Automated Design of Deep Learning Methods for Biomedical Image Segmentation <https://arxiv.org/abs/1904.08128>"_.
    "nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation <https://arxiv.org/abs/1809.10486>"_.
    "Optimized U-Net for Brain Tumor Segmentation <https://arxiv.org/pdf/2110.03352.pdf>"_.